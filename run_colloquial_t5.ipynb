{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional\n",
    "from pytorch_pretrained_bert.file_utils import WEIGHTS_NAME\n",
    "from transformers import RobertaTokenizer, RobertaConfig\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tensorboardX import SummaryWriter\n",
    "from model import  RobertaMoEForSequenceClassification\n",
    "import nltk\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tfidf_similarity import TfIdfSimilarity\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "LABELS = {\"SUPPORTS\":0, \"REFUTES\":1, \"NOT ENOUGH INFO\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, idx, text_a, text_b=None, label=None,priori=None):\n",
    "        '''\n",
    "        Args:\n",
    "            idx:   unique id\n",
    "            text_a: response/claim\n",
    "            text_b: context+evidence\n",
    "            label:  positive / negative / NEI\n",
    "            priori: priori distribution over experts based on rules\n",
    "        '''\n",
    "        self.idx = idx\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        self.priori = priori\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, priori):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.priori = priori\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    def get_examples(self, data_dir, dataset=None):\n",
    "        logger.info('Get examples from: {}.jsonl'.format(dataset))\n",
    "        return self._create_examples(self._get_json_lines(os.path.join(data_dir, \"{}.jsonl\".format(dataset))))\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [0, 1, 2], len([0, 1, 2])\n",
    "\n",
    "    def _get_json_lines(cls,inp_file):\n",
    "        lines = []\n",
    "        with jsonlines.open(inp_file) as reader:\n",
    "            for obj in reader:\n",
    "                lines.append(obj)\n",
    "                \n",
    "        return lines\n",
    "\n",
    "    def _create_examples(self, lines, max_evidences=5):\n",
    "        examples = []\n",
    "        obj = TfIdfSimilarity()\n",
    "        for i, datapoint in enumerate(tqdm(lines)):\n",
    "            if 'evidences' in datapoint.keys():\n",
    "                evidence = datapoint['evidences'][0]\n",
    "                evidence_text = 'title: ' + evidence['title'] + ' content: ' + evidence['evidence']\n",
    "                datapoint['evidence_touse'] = evidence_text\n",
    "            # if args.claim_only:\n",
    "            #     datapoint['evidence_touse'] = ''\n",
    "\n",
    "            #  sent1 = '[CONTEXT]: ' + ' [EOT] '.join(example['context'][-2:]) + ' [RESPONSE]: ' + sent1\n",
    "            primi_idx = datapoint['fever_id']\n",
    "            for claim in datapoint['colloquial_claims']:\n",
    "                text_a = claim\n",
    "                text_b = datapoint['question'] + datapoint['evidence_touse']\n",
    "                if 'fever_label' in datapoint.keys():\n",
    "                    label = LABELS[datapoint['fever_label']]\n",
    "\n",
    "                priori = get_priori(obj, text_a, datapoint['question'] ,datapoint['evidence_touse'], T = 1)\n",
    "                examples.append((InputExample(idx=primi_idx, text_a=text_a, text_b=text_b, label=label, priori=priori)))\n",
    "        return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for (ex_index, example) in enumerate(tqdm(examples, desc=\"convert to features\")):\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "        tokens = [\"<s>\"] + tokens_a + [\"</s>\"]\n",
    "        segment_ids = [0] * (len(tokens_a) + 2)\n",
    "        tokens += tokens_b + [\"</s>\"]\n",
    "        segment_ids += [1] * (len(tokens_b) + 1)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        padding = [1] * (max_seq_length - len(input_ids))\n",
    "        input_mask += [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        segment_ids += padding\n",
    "        #print(len(input_ids))\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 1:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(InputFeatures(input_ids=input_ids,\n",
    "                                      input_mask=input_mask,\n",
    "                                      segment_ids=segment_ids,\n",
    "                                      label_id=label_id,\n",
    "                                      priori=example.priori))\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def get_priori(obj, response, context, evidence, T = 1):\n",
    "    score = [0.2 ,0.2,0.6]\n",
    "    try:\n",
    "        res_ctx_score = 0.2*(1 - obj.cal_consine_similarities(response, context))\n",
    "        res_evi_score = 0.2*(1 - obj.cal_consine_similarities(response, evidence))\n",
    "    except:\n",
    "        res_ctx_score = 0\n",
    "        res_evi_score = 0\n",
    "    score[0] += res_ctx_score\n",
    "    score[1] += res_evi_score\n",
    "    score = softmax(score,T)\n",
    "    return score\n",
    "\n",
    "def compute_metrics_fn(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    f1 = f1_score(y_true= labels, y_pred=preds, average=\"macro\", labels=np.unique(labels))\n",
    "    acc = accuracy_score(y_true= labels, y_pred=preds)\n",
    "    p = precision_score(y_true= labels, y_pred=preds, average=\"macro\", labels=np.unique(labels))\n",
    "    r = recall_score(y_true= labels, y_pred=preds, average=\"macro\", labels=np.unique(labels))\n",
    "    return {\n",
    "        \"p\": p,\n",
    "        \"acc\": acc,\n",
    "        \"macro_f1\": f1,\n",
    "        \"macro_recall\":r\n",
    "    }\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def get_dataLoader(args, processor, tokenizer, phase=None):\n",
    "    dataset_dict = {\"train\": args.train_set, \"dev\": args.dev_set, \"test\": args.test_set}\n",
    "    label_list, _ = processor.get_labels()\n",
    "\n",
    "    examples = processor.get_examples(args.data_dir, dataset_dict[phase])\n",
    "    features = convert_examples_to_features(examples, label_list, args.max_seq_length, tokenizer)\n",
    "\n",
    "    batch_size = args.train_batch_size if phase == \"train\" else args.eval_batch_size\n",
    "    epoch_num = args.num_train_epochs if phase == \"train\" else 1\n",
    "    num_optimization_steps = int(len(examples) / batch_size / args.gradient_accumulation_steps) * epoch_num\n",
    "    logger.info(\"Examples#: {}, Batch size: {}\".format(len(examples), batch_size * args.gradient_accumulation_steps))\n",
    "    logger.info(\"Total num of steps#: {}, Total num of epoch#: {}\".format(num_optimization_steps, epoch_num))\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    all_priori = torch.tensor([f.priori for f in features], dtype=torch.float)\n",
    "\n",
    "    all_data = TensorDataset(all_input_ids, all_input_mask, all_label_ids, all_priori)\n",
    "    if args.do_train_eval:\n",
    "        sampler = SequentialSampler(all_data)\n",
    "    else:\n",
    "        sampler = RandomSampler(all_data) if phase == \"train\" else SequentialSampler(all_data)\n",
    "    dataloader = DataLoader(all_data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return dataloader, num_optimization_steps, examples\n",
    "\n",
    "\n",
    "def save_model(model_to_save):\n",
    "    save_model_dir = os.path.join(args.output_dir, 'saved_model')\n",
    "    mkdir(save_model_dir)\n",
    "    output_model_file = os.path.join(save_model_dir, WEIGHTS_NAME)\n",
    "    # output_config_file = os.path.join(save_model_dir, CONFIG_NAME)\n",
    "    torch.save(model_to_save.state_dict(), output_model_file, _use_new_zipfile_serialization=False)\n",
    "    # model_to_save.config.to_json_file(output_config_file)\n",
    "    # tokenizer.save_vocabulary(save_model_dir)\n",
    "\n",
    "def softmax(input,T=1):\n",
    "    output = [np.exp(i/T) for i in input]\n",
    "    output_sum = sum(output)\n",
    "    final = [i/output_sum for i in output]\n",
    "    return final\n",
    "\n",
    "def is_count_number(num):\n",
    "    return 0 <= num <= 10\n",
    "\n",
    "def run_train(device, processor, tokenizer, model, writer, phase=\"train\"):\n",
    "    logger.info(\"\\n************ Start Training *************\")\n",
    "\n",
    "    tr_dataloader, tr_num_steps, tr_examples = get_dataLoader(args, processor, tokenizer, phase=\"train\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_fct = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = \\\n",
    "        [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                                 lr=args.learning_rate,\n",
    "                                 warmup=args.warmup_proportion,\n",
    "                                 t_total=tr_num_steps)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    global_step = 0\n",
    "    best_acc = 0.0\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    for ep in trange(args.num_train_epochs, desc=\"Training\"):\n",
    "        for step, batch in tqdm(enumerate(tr_dataloader)):\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, label_ids, priori = batch\n",
    "            logits, loss, final_out_logits, origin_gates = model(input_ids=input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "            guide_loss = loss_fct(torch.nn.functional.log_softmax(origin_gates, dim=1), priori)\n",
    "            loss += args.lmd * guide_loss\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            writer.add_scalar('{}/loss'.format(phase), loss.item(), global_step)\n",
    "\n",
    "            loss.backward()\n",
    "            del loss\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:  # optimizer\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "            model.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "\n",
    "            if args.do_eval and (((step + 1) % args.gradient_accumulation_steps == 0 and global_step % args.period == 0) or (ep==0 and step==0)):\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "                dev_acc, dev_recall = run_eval(device, processor, tokenizer, model, writer, global_step, tensorboard=True,\n",
    "                                   phase=\"dev\")\n",
    "                if dev_acc > best_acc:\n",
    "                    best_acc = dev_acc\n",
    "                    logger.info(\">> Save model. Best acc: {:.4}. Epoch {}\".format(best_acc, ep))\n",
    "                    save_model(model_to_save)  # save model\n",
    "                    logger.info(\">> Now the best acc is {:.4}\\n, recall is {:.4}\".format(dev_acc, dev_recall))\n",
    "\n",
    "            model.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "\n",
    "    return global_step\n",
    "\n",
    "\n",
    "def run_eval(device, processor, tokenizer, model, writer, global_step, tensorboard=False,\n",
    "             phase=None):\n",
    "    sys.stdout.flush()\n",
    "    logger.info(\"\\n************ Start {} *************\".format(phase))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_fct = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    cross_entropy = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    dataloader, num_steps, examples = get_dataLoader(args, processor, tokenizer, phase=phase)\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    eval_guide_loss = 0.0\n",
    "    num_steps = 0\n",
    "    preds = []\n",
    "    preds_0, preds_1, preds_2= [],[],[]\n",
    "    all_labels = []\n",
    "    mapping = []\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=phase)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, label_ids, priori = batch\n",
    "        num_steps += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits, tmp_loss, final_out_logits, origin_gates = model(input_ids=input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "            guide_loss = loss_fct(torch.nn.functional.log_softmax(origin_gates, dim=1), priori)\n",
    "\n",
    "            eval_loss += tmp_loss.mean().item()\n",
    "            eval_guide_loss += guide_loss.mean().item()\n",
    "            logits_sigmoid = final_out_logits\n",
    "            loss = []\n",
    "            for l in logits:\n",
    "                loss.append(cross_entropy(l.squeeze(1), label_ids.view(-1)).view(-1,1))\n",
    "            if len(loss) == 1:\n",
    "                loss_mat = loss[0].view(-1,1)\n",
    "            else:\n",
    "                loss_mat = torch.cat(loss, dim=1) # bsz * # of experts\n",
    "            logits_sigmoid_0 = torch.nn.functional.softmax(logits[0].squeeze(1), dim=1)\n",
    "            logits_sigmoid_1 = torch.nn.functional.softmax(logits[1].squeeze(1), dim=1)\n",
    "            logits_sigmoid_2 = torch.nn.functional.softmax(logits[2].squeeze(1), dim=1)\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits_sigmoid.detach().cpu().numpy())\n",
    "                preds_0.append(logits_sigmoid_0.detach().cpu().numpy())\n",
    "                preds_1.append(logits_sigmoid_1.detach().cpu().numpy())\n",
    "                preds_2.append(logits_sigmoid_2.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = np.append(preds[0], logits_sigmoid.detach().cpu().numpy(), axis=0)\n",
    "                preds_0[0] = np.append(preds_0[0], logits_sigmoid_0.detach().cpu().numpy(), axis=0)\n",
    "                preds_1[0] = np.append(preds_1[0], logits_sigmoid_1.detach().cpu().numpy(), axis=0)\n",
    "                preds_2[0] = np.append(preds_2[0], logits_sigmoid_2.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            labels = label_ids.detach().cpu().numpy().tolist()\n",
    "\n",
    "            start = step * args.eval_batch_size if not args.do_train_eval else step * args.train_batch_size\n",
    "            end = start + len(labels)\n",
    "            batch_range = list(range(start, end))\n",
    "\n",
    "            idx = [examples[i].idx for i in batch_range]\n",
    "            labels = label_ids.detach().cpu().numpy().tolist()\n",
    "            all_labels.extend(labels)\n",
    "            loss_mat_cpu = loss_mat.detach().cpu().numpy().tolist()\n",
    "            for i, t_name in enumerate(idx):\n",
    "                mapping.append([str(loss_mat_cpu[i][0]), str(loss_mat_cpu[i][1]), str(loss_mat_cpu[i][2])])\n",
    "\n",
    "    result = {}\n",
    "    result['acc'] = 0\n",
    "    eval_loss /= num_steps\n",
    "    eval_guide_loss /= num_steps\n",
    "    preds = np.argmax(preds[0], axis=1)\n",
    "    preds_0 = np.argmax(preds_0[0], axis=1)\n",
    "    preds_1 = np.argmax(preds_1[0], axis=1)\n",
    "    preds_2 = np.argmax(preds_2[0], axis=1)\n",
    "    pred_for_test, label_for_test = [] ,[]\n",
    "    for pred, label in zip(preds,all_labels):\n",
    "        pred_for_test.append(pred)\n",
    "        label_for_test.append(label)\n",
    "            \n",
    "    result = compute_metrics_fn(np.asarray(pred_for_test), np.asarray(label_for_test))\n",
    "    result_0 = compute_metrics_fn(np.asarray(preds_0), np.asarray(all_labels))\n",
    "    result_1 = compute_metrics_fn(np.asarray(preds_1), np.asarray(all_labels))\n",
    "    result_2 = compute_metrics_fn(np.asarray(preds_2), np.asarray(all_labels))\n",
    "    result['acc_0'] = result_0['acc']\n",
    "    result['acc_1'] = result_1['acc']\n",
    "    result['acc_2'] = result_2['acc']\n",
    "    result['{}_loss'.format(phase)] = eval_loss\n",
    "    result['{}_guide_loss'.format(phase)] = eval_guide_loss\n",
    "    result['global_step'] = global_step\n",
    "    logger.info(result)\n",
    "    if tensorboard and writer is not None:\n",
    "        for key in sorted(result.keys()):\n",
    "            writer.add_scalar('{}/{}'.format(phase, key), result[key], global_step)\n",
    "    json.dump(mapping, open('./{}_moe_roberta_lmd_0.1.json'.format(phase),'w', encoding='utf8'))\n",
    "        \n",
    "    model.train()\n",
    "    return result['acc'], result['macro_recall']\n",
    "\n",
    "\n",
    "def main():\n",
    "    mkdir(args.output_dir)\n",
    "\n",
    "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "    writer = SummaryWriter(os.path.join(args.output_dir, 'events'))\n",
    "    cache_dir = args.cache_dir\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    save_code_log_path = args.output_dir\n",
    "\n",
    "    logging.basicConfig(format='%(message)s', datefmt='%m/%d/%Y %H:%M', level=logging.INFO,\n",
    "                        handlers=[logging.FileHandler(\"{0}/{1}.log\".format(save_code_log_path, 'output')),\n",
    "                                  logging.StreamHandler()])\n",
    "    logger.info(args)\n",
    "    logger.info(\"Command is: %s\" % ' '.join(sys.argv))\n",
    "    logger.info(\"Device: {}, n_GPU: {}\".format(device, n_gpu))\n",
    "    logger.info(\"Datasets are loaded from {}\\nOutputs will be saved to {}\\n\".format(args.data_dir, args.output_dir))\n",
    "\n",
    "    processor = DataProcessor()\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    load_dir = args.load_dir if args.load_dir else args.bert_model\n",
    "    logger.info('Model is loaded from %s' % load_dir)\n",
    "    label_list = processor.get_labels()\n",
    "    config = RobertaConfig.from_json_file(os.path.join(args.bert_model,'config.json'))\n",
    "    model = RobertaMoEForSequenceClassification(config, num_public_layers=12, num_experts=3,num_labels=3, num_gate_layer=2)\n",
    "    model.load_roberta(args.bert_model)\n",
    "    if args.load_dir:\n",
    "        model.load_state_dict(torch.load(load_dir+'/pytorch_model.bin'))\n",
    "        print('parameters loaded successfully.')\n",
    "    model.to(device)\n",
    "\n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model,device_ids=[0, 1])\n",
    "\n",
    "    if args.do_train:\n",
    "        run_train(device, processor, tokenizer, model, writer, phase=\"train\")\n",
    "\n",
    "    if args.do_eval:\n",
    "        run_eval(device, processor, tokenizer, model, writer, global_step=0, tensorboard=False,\n",
    "                 phase=\"dev\")\n",
    "        run_eval(device, processor, tokenizer, model, writer, global_step=0, tensorboard=False,\n",
    "                 phase=\"test\")\n",
    "\n",
    "    if args.do_test:\n",
    "        run_eval(device, processor, tokenizer, model, writer, global_step=0, tensorboard=False,\n",
    "                 phase=\"test\")\n",
    "\n",
    "    if args.do_train_eval:\n",
    "        run_eval(device, processor, tokenizer, model, writer, global_step=0, tensorboard=False,\n",
    "                 phase=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"-f\")\n",
    "\n",
    "parser.add_argument(\"--do_train\", action='store_true')\n",
    "parser.add_argument(\"--do_eval\", action='store_true')\n",
    "parser.add_argument(\"--do_test\", action='store_true')\n",
    "parser.add_argument(\"--do_train_eval\", action='store_true')\n",
    "parser.add_argument(\"--add_unk\", action='store_true')\n",
    "parser.add_argument(\"--load_dir\", help=\"load model checkpoints\")\n",
    "#parser.add_argument(\"--data_dir\", help=\"path to data\", default='../data/colloquial')\n",
    "#parser.add_argument(\"--data_dir\", help=\"path to data\", default='./colloquial-claims/data') ##다운받은 후 세 파일 이름도 수정완료\n",
    "parser.add_argument(\"--data_dir\", help=\"path to data\", default=os.path.abspath(os.path.join(os.getcwd(), '..', '..', 'colloquial-claims/data')))\n",
    "parser.add_argument(\"--train_set\", default=\"colloquial_claims_train_t5\")\n",
    "parser.add_argument(\"--dev_set\", default=\"colloquial_claims_valid_t5\")\n",
    "parser.add_argument(\"--test_set\", default=\"colloquial_claims_test_t5\")\n",
    "parser.add_argument(\"--output_dir\", default='./outputs_colloquial_t5')\n",
    "parser.add_argument(\"--cache_dir\", default=\"./roberta\", type=str, help=\"store downloaded pre-trained models\")\n",
    "parser.add_argument('--period', type=int, default=1000)\n",
    "#parser.add_argument(\"--bert_model\", default=\"../roberta_large\", type=str)\n",
    "parser.add_argument(\"--bert_model\", default=\"./roberta-large\", type=str)\n",
    "parser.add_argument(\"--do_lower_case\", default=True, help=\"Set this flag if you are using an uncased model.\")\n",
    "parser.add_argument(\"--task_name\", default=\"LPA\", type=str)\n",
    "parser.add_argument('--response_tag', type=str, help='tag', default='response')\n",
    "parser.add_argument(\"--max_seq_length\", default=512)\n",
    "parser.add_argument(\"--train_batch_size\", default=32)\n",
    "parser.add_argument(\"--eval_batch_size\", default=32)\n",
    "parser.add_argument('--debug_mode', action='store_true')\n",
    "parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=20)\n",
    "#parser.add_argument(\"--num_train_epochs\", default=1)\n",
    "parser.add_argument(\"--lmd\",default=0.1, type=float, help=\"the ratio of guide loss in the ttl loss\")\n",
    "parser.add_argument(\"--warmup_proportion\", default=0.3, type=float, help=\"0.1 = 10%% of training.\")\n",
    "parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n",
    "parser.add_argument('--seed', type=int, default=42, help=\"random seed\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Namespace(add_unk=False, bert_model='./roberta-large', cache_dir='./roberta', data_dir='/data/home/acw722/STAGE0/colloquial-claims/data', debug_mode=False, dev_set='colloquial_claims_valid_t5', do_eval=False, do_lower_case=True, do_test=False, do_train=False, do_train_eval=False, eval_batch_size=32, f='/data/home/acw722/.local/share/jupyter/runtime/kernel-c4a2b8d1-7828-4a64-a74d-539f591f1e69.json', gradient_accumulation_steps=1, learning_rate=2e-05, lmd=0.1, load_dir=None, max_seq_length=512, num_train_epochs=20, output_dir='./outputs_colloquial_t5', period=1000, response_tag='response', seed=42, task_name='LPA', test_set='colloquial_claims_test_t5', train_batch_size=32, train_set='colloquial_claims_train_t5', warmup_proportion=0.3)\n",
      "Command is: /data/home/acw722/pipenv/lib/python3.8/site-packages/ipykernel_launcher.py -f /data/home/acw722/.local/share/jupyter/runtime/kernel-c4a2b8d1-7828-4a64-a74d-539f591f1e69.json\n",
      "Device: cuda, n_GPU: 0\n",
      "Datasets are loaded from /data/home/acw722/STAGE0/colloquial-claims/data\n",
      "Outputs will be saved to ./outputs_colloquial_t5\n",
      "\n",
      "Model is loaded from ./roberta-large\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.pooler.dense.weight\n",
      "roberta.pooler.dense.bias\n",
      "lm_head.bias\n",
      "lm_head.dense.weight\n",
      "lm_head.dense.bias\n",
      "lm_head.layer_norm.weight\n",
      "lm_head.layer_norm.bias\n",
      "lm_head.decoder.weight\n",
      "roberta loaded successfully.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[2], line 405\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    403\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(load_dir\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/pytorch_model.bin\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    404\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mparameters loaded successfully.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 405\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m n_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    408\u001b[0m     model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDataParallel(model,device_ids\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/pipenv/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/pipenv/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/pipenv/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 641 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/pipenv/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/pipenv/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/pipenv/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/pipenv/lib/python3.8/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    230\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: args\u001b[39m.\u001b[39mtrain_set, \u001b[39m\"\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m\"\u001b[39m: args\u001b[39m.\u001b[39mdev_set, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m: args\u001b[39m.\u001b[39mtest_set}\n\u001b[1;32m      2\u001b[0m label_list, _ \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mget_labels()\n\u001b[0;32m----> 4\u001b[0m examples \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mget_examples(args\u001b[39m.\u001b[39mdata_dir, dataset_dict[phase])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'phase' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_dict = {\"train\": args.train_set, \"dev\": args.dev_set, \"test\": args.test_set}\n",
    "label_list, _ = processor.get_labels()\n",
    "\n",
    "examples = processor.get_examples(args.data_dir, dataset_dict[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
